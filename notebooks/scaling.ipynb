{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q transformers datasets evaluate accelerate gluonts ujson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Fin/Finance/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# from loader.custom_dataset import DS, collate_func\n",
    "from glob import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gluonts.time_feature import get_lags_for_frequency, time_features_from_frequency_str\n",
    "from datasets import load_dataset, Dataset, DatasetDict, DatasetInfo\n",
    "\n",
    "import os\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# custom gluonts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'datetime.datetime' and 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mcombine(start_time\u001b[38;5;241m.\u001b[39mdate(), time(\u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     40\u001b[0m x \u001b[38;5;241m=\u001b[39m df[(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSTCK_CNTG_HOUR\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m start_time) \u001b[38;5;241m&\u001b[39m (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSTCK_CNTG_HOUR\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m end_time)]\n\u001b[0;32m---> 41\u001b[0m y \u001b[38;5;241m=\u001b[39m df[(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSTCK_CNTG_HOUR\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m end_time) \u001b[38;5;241m&\u001b[39m (df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTCK_CNTG_HOUR\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mend_time\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtarget_size\u001b[49m)]\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# 다음 슬라이딩 시작 시간으로 업데이트\u001b[39;00m\n\u001b[1;32m     44\u001b[0m start_time \u001b[38;5;241m=\u001b[39m end_time\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'datetime.datetime' and 'tuple'"
     ]
    }
   ],
   "source": [
    "# OHLC 변환 및 저장\n",
    "from datetime import time, datetime\n",
    "parquet_files = glob(os.path.join(os.path.abspath(os.path.pardir), \"data\", \"raw\", \"*.parquet\"))\n",
    "\n",
    "freq = \"1s\"\n",
    "prediction_length = 48\n",
    "\n",
    "window_size=(3, 0)\n",
    "sliding_size=(0, 30)\n",
    "target_size=(1, 0)\n",
    "preproc = []\n",
    "def calculate_prob_distribution(data):\n",
    "    # 1% 상승한 포인트 수\n",
    "    true_count = np.sum(data)\n",
    "\n",
    "    # True의 확률\n",
    "    true_prob = true_count / data.size\n",
    "\n",
    "    # False의 확률\n",
    "    false_prob = 1 - true_prob\n",
    "    \n",
    "    return true_prob, false_prob # input의 클래스 확률 분포 (True, False)\n",
    "\n",
    "for idx, parquet_file in enumerate(parquet_files):\n",
    "    try:\n",
    "        start_idx = 0\n",
    "        df = pd.read_parquet(parquet_file)\n",
    "        start_time = df['STCK_CNTG_HOUR'].iloc[0]\n",
    "        date_list = df['STCK_CNTG_HOUR'].dt.date.unique()\n",
    "        \n",
    "    except IndexError:\n",
    "        raise StopIteration\n",
    "    \n",
    "    except StopIteration:\n",
    "        break\n",
    "    \n",
    "    if start_time.time() < time(9, 0, 0):\n",
    "        end_time = datetime.combine(start_time.date(), time(9, 0, 0))\n",
    "        \n",
    "        x = df[(df['STCK_CNTG_HOUR'] >= start_time) & (df['STCK_CNTG_HOUR'] < end_time)]\n",
    "        y = df[(df['STCK_CNTG_HOUR'] >= end_time) & (df[\"STCK_CNTG_HOUR\"] <= end_time + target_size)]\n",
    "        \n",
    "        # 다음 슬라이딩 시작 시간으로 업데이트\n",
    "        start_time = end_time\n",
    "        # 다음 슬라이딩 윈도우의 시작 인덱스 업데이트\n",
    "        start_idx = df.index[df['STCK_CNTG_HOUR'] >= start_time].min()\n",
    "        \n",
    "    elif start_time >= datetime.combine(start_time.date(), time(15, 20, 0)) - target_size: # \n",
    "        try:\n",
    "            end_time = datetime.combine(date_list[np.where(date_list > start_time.date())][0], time(9, 0, 0))\n",
    "        except IndexError:\n",
    "            break\n",
    "        \n",
    "        x = df[(df['STCK_CNTG_HOUR'] >= start_time) & (df['STCK_CNTG_HOUR'] < end_time)]\n",
    "        y = df[(df['STCK_CNTG_HOUR'] >= end_time) & (df[\"STCK_CNTG_HOUR\"] <= end_time + target_size)]\n",
    "        \n",
    "        # 다음 슬라이딩 시작 시간으로 업데이트\n",
    "        start_time = end_time\n",
    "        # 다음 슬라이딩 윈도우의 시작 인덱스 업데이트\n",
    "        start_idx = df.index[df['STCK_CNTG_HOUR'] >= start_time].min()\n",
    "\n",
    "    else:\n",
    "        end_time = start_time + window_size\n",
    "        x = df[(df['STCK_CNTG_HOUR'] >= start_time) & (df['STCK_CNTG_HOUR'] < end_time)]\n",
    "        y = df[(df['STCK_CNTG_HOUR'] >= end_time) & (df['STCK_CNTG_HOUR'] <= end_time + target_size)]\n",
    "        \n",
    "        # 다음 슬라이딩 시작 시간으로 업데이트\n",
    "        start_time += sliding_size\n",
    "        # 다음 슬라이딩 윈도우의 시작 인덱스 업데이트\n",
    "        start_idx = df.index[df['STCK_CNTG_HOUR'] >= start_time].min()\n",
    "            \n",
    "    if x.empty:\n",
    "        continue\n",
    "    \n",
    "    y = calculate_prob_distribution(((y[\"STCK_PRPR\"] - x.iloc[-1][\"STCK_PRPR\"]) / x.iloc[-1][\"STCK_PRPR\"] >= 0.01).values)\n",
    "    preproc.append([x, y])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OHLC 변환 및 저장\n",
    "from datetime import time\n",
    "parquet_files = glob(os.path.join(os.path.abspath(os.path.pardir), \"data\", \"raw\", \"*.parquet\"))\n",
    "\n",
    "freq = \"1s\"\n",
    "prediction_length = 48\n",
    "\n",
    "window_size=(3, 0)\n",
    "sliding_size=(0, 30)\n",
    "target_size=(1, 0)\n",
    "\n",
    "for idx, parquet_file in enumerate(parquet_files):\n",
    "    df = pd.read_parquet(parquet_file)\n",
    "    df = df.assign(ask_volume=df[(df[\"CCLD_DVSN\"] == 1) & (df[\"STCK_CNTG_HOUR\"].dt.time < time(15,20,0)) & (df[\"STCK_CNTG_HOUR\"].dt.time >= time(9,0,0))][\"CNTG_VOL\"]).fillna(0)\n",
    "    df = df.assign(bid_volume=df[(df[\"CCLD_DVSN\"] == 5) & (df[\"STCK_CNTG_HOUR\"].dt.time < time(15,20,0)) & (df[\"STCK_CNTG_HOUR\"].dt.time >= time(9,0,0))][\"CNTG_VOL\"]).fillna(0)\n",
    "    df = df.assign(after_volume=df[(df[\"CCLD_DVSN\"] == 3) | (df[\"STCK_CNTG_HOUR\"].dt.time >= time(15,20,0)) | (df[\"STCK_CNTG_HOUR\"].dt.time < time(9,0,0))][\"CNTG_VOL\"]).fillna(0)\n",
    "\n",
    "    resampler = df.resample(on=\"STCK_CNTG_HOUR\", rule=\"1s\")\n",
    "    \n",
    "    re = resampler.agg({\n",
    "        \"STCK_PRPR\": \"ohlc\",\n",
    "        \"CNTG_VOL\" : \"sum\",\n",
    "        \"ask_volume\" : \"sum\",\n",
    "        \"bid_volume\" : \"sum\",\n",
    "        \"after_volume\" : \"sum\"})\n",
    "    \n",
    "    re.columns = [col[1] for col in re.columns] # 튜플 colunm 해체\n",
    "    re[[\"open\", \"high\", \"close\", \"low\"]] = re[[\"open\", \"high\", \"close\", \"low\"]].ffill()\n",
    "    \n",
    "    re = re.assign(ticks=resampler.size())\n",
    "    re = re[(re.index.time >= time(8,30,0))&(re.index.time <= time(16,0,0))]\n",
    "\n",
    "    re[\"start\"] = re[re.index < (re.index + pd.Timedelta(seconds=180))]\n",
    "\n",
    "    re.to_parquet(parquet_file.replace(\"raw\", \"resample\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Sampling cell, 중복된 timestamp에 대해 ms값 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_unique_timestamps(group):\n",
    "    if len(group) > 1:  # Check if there are duplicates\n",
    "        # Generate a range of microseconds to add, ensuring uniqueness within the group\n",
    "        microseconds = pd.to_timedelta(np.arange(len(group))*len(group), unit='ms')\n",
    "        # print(\"##############us##############\\n\", microseconds)\n",
    "        # print(\"##############us basic##############\\n\",pd.to_timedelta(np.arange(len(group)), unit='ms'))\n",
    "        group['STCK_CNTG_HOUR'] += microseconds\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       MKSC_SHRN_ISCD      STCK_CNTG_HOUR  STCK_PRPR  CNTG_VOL  CCLD_DVSN\n",
      "0              005490 2023-09-11 08:30:00   583000.0       1.0        3.0\n",
      "1              005490 2023-09-11 08:30:00   583000.0       1.0        3.0\n",
      "2              005490 2023-09-11 08:30:00   583000.0      30.0        3.0\n",
      "3              005490 2023-09-11 08:30:00   583000.0       3.0        3.0\n",
      "4              005490 2023-09-11 08:30:00   583000.0       3.0        3.0\n",
      "...               ...                 ...        ...       ...        ...\n",
      "714727         005490 2023-10-04 15:56:55   511000.0       5.0        5.0\n",
      "714728         005490 2023-10-04 15:57:42   511000.0       3.0        5.0\n",
      "714729         005490 2023-10-04 15:57:55   511000.0      10.0        5.0\n",
      "714730         005490 2023-10-04 15:58:39   511000.0       1.0        5.0\n",
      "714731         005490 2023-10-04 15:58:46   511000.0       4.0        5.0\n",
      "\n",
      "[714732 rows x 5 columns]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(parquet_file)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n\u001b[0;32m----> 9\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSTCK_CNTG_HOUR\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerate_unique_timestamps\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Fin/Finance/.venv/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1770\u001b[0m, in \u001b[0;36mGroupBy.apply\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1768\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1769\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1770\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_apply_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selected_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   1772\u001b[0m         \u001b[38;5;66;03m# gh-20949\u001b[39;00m\n\u001b[1;32m   1773\u001b[0m         \u001b[38;5;66;03m# try again, with .apply acting as a filtering\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1777\u001b[0m         \u001b[38;5;66;03m# fails on *some* columns, e.g. a numeric operation\u001b[39;00m\n\u001b[1;32m   1778\u001b[0m         \u001b[38;5;66;03m# on a string grouper column\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_python_apply_general(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj_with_exclusions)\n",
      "File \u001b[0;32m~/Fin/Finance/.venv/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1819\u001b[0m, in \u001b[0;36mGroupBy._python_apply_general\u001b[0;34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[0m\n\u001b[1;32m   1784\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   1785\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_python_apply_general\u001b[39m(\n\u001b[1;32m   1786\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1791\u001b[0m     is_agg: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1792\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;124;03m    Apply function f in python space\u001b[39;00m\n\u001b[1;32m   1795\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1817\u001b[0m \u001b[38;5;124;03m        data after applying f\u001b[39;00m\n\u001b[1;32m   1818\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1819\u001b[0m     values, mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrouper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_groupwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1820\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1821\u001b[0m         not_indexed_same \u001b[38;5;241m=\u001b[39m mutated\n",
      "File \u001b[0;32m~/Fin/Finance/.venv/lib/python3.11/site-packages/pandas/core/groupby/ops.py:911\u001b[0m, in \u001b[0;36mBaseGrouper.apply_groupwise\u001b[0;34m(self, f, data, axis)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;66;03m# group might be modified\u001b[39;00m\n\u001b[1;32m    910\u001b[0m group_axes \u001b[38;5;241m=\u001b[39m group\u001b[38;5;241m.\u001b[39maxes\n\u001b[0;32m--> 911\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mutated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n\u001b[1;32m    913\u001b[0m     mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m, in \u001b[0;36mgenerate_unique_timestamps\u001b[0;34m(group)\u001b[0m\n\u001b[1;32m      4\u001b[0m     microseconds \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_timedelta(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(group))\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(group), unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mms\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# print(\"##############us##############\\n\", microseconds)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# print(\"##############us basic##############\\n\",pd.to_timedelta(np.arange(len(group)), unit='ms'))\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSTCK_CNTG_HOUR\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m microseconds\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m group\n",
      "File \u001b[0;32m~/Fin/Finance/.venv/lib/python3.11/site-packages/pandas/core/frame.py:4091\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4088\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   4089\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4090\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[0;32m-> 4091\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Fin/Finance/.venv/lib/python3.11/site-packages/pandas/core/frame.py:4314\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4311\u001b[0m             value \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtile(value, (\u001b[38;5;28mlen\u001b[39m(existing_piece\u001b[38;5;241m.\u001b[39mcolumns), \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m   4312\u001b[0m             refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 4314\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Fin/Finance/.venv/lib/python3.11/site-packages/pandas/core/frame.py:4266\u001b[0m, in \u001b[0;36mDataFrame._set_item_mgr\u001b[0;34m(self, key, value, refs)\u001b[0m\n\u001b[1;32m   4264\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis), key, value, refs)\n\u001b[1;32m   4265\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4266\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iset_item_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4268\u001b[0m \u001b[38;5;66;03m# check if we are modifying a copy\u001b[39;00m\n\u001b[1;32m   4269\u001b[0m \u001b[38;5;66;03m# try to set first as we want an invalid\u001b[39;00m\n\u001b[1;32m   4270\u001b[0m \u001b[38;5;66;03m# value exception to occur first\u001b[39;00m\n\u001b[1;32m   4271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "File \u001b[0;32m~/Fin/Finance/.venv/lib/python3.11/site-packages/pandas/core/frame.py:4254\u001b[0m, in \u001b[0;36mDataFrame._iset_item_mgr\u001b[0;34m(self, loc, value, inplace, refs)\u001b[0m\n\u001b[1;32m   4246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_iset_item_mgr\u001b[39m(\n\u001b[1;32m   4247\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4248\u001b[0m     loc: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mslice\u001b[39m \u001b[38;5;241m|\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4252\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4253\u001b[0m     \u001b[38;5;66;03m# when called from _set_item_mgr loc can be anything returned from get_loc\u001b[39;00m\n\u001b[0;32m-> 4254\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n",
      "File \u001b[0;32m~/Fin/Finance/.venv/lib/python3.11/site-packages/pandas/core/internals/managers.py:1068\u001b[0m, in \u001b[0;36mBlockManager.iset\u001b[0;34m(self, loc, value, inplace, refs)\u001b[0m\n\u001b[1;32m   1066\u001b[0m     value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1068\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[43mensure_block_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m   1071\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m   1072\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of new values must be compatible with manager shape\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1073\u001b[0m     )\n",
      "File \u001b[0;32m~/Fin/Finance/.venv/lib/python3.11/site-packages/pandas/core/internals/blocks.py:2494\u001b[0m, in \u001b[0;36mensure_block_shape\u001b[0;34m(values, ndim)\u001b[0m\n\u001b[1;32m   2489\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_1d_only_ea_dtype(values\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m   2490\u001b[0m         \u001b[38;5;66;03m# TODO(EA2D): https://github.com/pandas-dev/pandas/issues/23023\u001b[39;00m\n\u001b[1;32m   2491\u001b[0m         \u001b[38;5;66;03m# block.shape is incorrect for \"2D\" ExtensionArrays\u001b[39;00m\n\u001b[1;32m   2492\u001b[0m         \u001b[38;5;66;03m# We can't, and don't need to, reshape.\u001b[39;00m\n\u001b[1;32m   2493\u001b[0m         values \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.ndarray | DatetimeArray | TimedeltaArray\u001b[39m\u001b[38;5;124m\"\u001b[39m, values)\n\u001b[0;32m-> 2494\u001b[0m         values \u001b[38;5;241m=\u001b[39m \u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "parquet_files = glob(os.path.join(os.path.abspath(os.path.pardir), \"data\", \"raw\", \"*.parquet\"))\n",
    "\n",
    "\n",
    "for idx, parquet_file in enumerate(sorted(parquet_files)):\n",
    "    df = pd.read_parquet(parquet_file)\n",
    "    print(df)\n",
    "    df = df.groupby(\"STCK_CNTG_HOUR\").apply(generate_unique_timestamps).reset_index(drop=True)\n",
    "    print(df)\n",
    "    break\n",
    "    df.to_parquet(parquet_file.replace(\"raw\", \"pre\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'target' 열을 계산하는 함수\n",
    "from datetime import datetime, time\n",
    "\n",
    "def compute_target(row):\n",
    "    if row[\"start\"].time() < time(9,0,0):\n",
    "        src_start = row[\"start\"]\n",
    "        src_end = datetime.combine(src_start.date(), time(9,0,0))\n",
    "        trg_start = src_end\n",
    "        trg_end = trg_start + pd.Timedelta(seconds=30)\n",
    "        \n",
    "    elif row[\"start\"].time() >= time(15, 20, 0):\n",
    "        src_start = row[\"start\"]\n",
    "        src_end = datetime.combine(src_start.date() + pd.Timedelta(days=1), time(9,0,0))\n",
    "        trg_start = src_end\n",
    "        trg_end = trg_start + pd.Timedelta(seconds=30)  # 그로부터 1분 후\n",
    "    else:\n",
    "        src_start = row[\"start\"]\n",
    "        src_end = src_start + pd.Timedelta(minutes=3)\n",
    "        trg_start = src_end  # 각 행의 시간으로부터 3분 후\n",
    "        trg_end = trg_start + pd.Timedelta(seconds=30)\n",
    "\n",
    "    # trg_start과 trg_end 사이의 데이터 필터링\n",
    "    src_mask = (df['start'] >= src_start) & (df['start'] <= src_end)\n",
    "    trg_mask = (df['start'] > trg_start) & (df['start'] < trg_end)\n",
    "    \n",
    "    filtered_src = df.loc[src_mask].values if len(df.loc[src_mask]) > 0 else np.array([None])\n",
    "    filtered_trg = df.loc[trg_mask][\"per\"].values if len(df.loc[trg_mask][\"per\"]) > 0 else np.array([0.])\n",
    "\n",
    "    # 필터링된 데이터를 리스트로 변환하여 반환\n",
    "    return (filtered_src, filtered_trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_files = glob(os.path.join(os.path.abspath(os.path.pardir), \"data\", \"raw\", \"*.parquet\"))\n",
    "\n",
    "dataset = DatasetDict({'train': {}, 'validation': {}, \"test\": {}})\n",
    "    \n",
    "##############\n",
    "freq = \"1s\"\n",
    "lags_sequence = get_lags_for_frequency(freq)\n",
    "time_features = time_features_from_frequency_str(freq)\n",
    "##############\n",
    "\n",
    "fig = make_subplots(rows=6, cols=1)\n",
    "\n",
    "for idx, parquet_file in enumerate(parquet_files):\n",
    "    df = pd.read_parquet(parquet_file)\n",
    "    df.rename(columns={\"STCK_CNTG_HOUR\":\"start\"}, inplace=True)\n",
    "    df[\"per\"] = df[\"STCK_PRPR\"] - df[\"STCK_PRPR\"].shift(fill_value=0)\n",
    "    df[\"STCK_PRPR\"] = df[\"STCK_PRPR\"] * df[\"CNTG_VOL\"]\n",
    "    df.rename(columns={\"STCK_PRPR\":\"amount\"}, inplace=True)\n",
    "    src, trg = df.apply(compute_target, axis=1)\n",
    "    print(src)\n",
    "    print(trg)\n",
    "    break\n",
    "    # df[\"src\"] = src\n",
    "    # df[\"trg\"] = trg\n",
    "\n",
    "    # 시계열 Features\n",
    "    timestamp_as_index = pd.DatetimeIndex(df[\"start\"])\n",
    "    additional_features = [\n",
    "        (time_feature.__name__, time_feature(timestamp_as_index))\n",
    "        for time_feature in time_features\n",
    "    ]\n",
    "    \n",
    "    time_features = pd.DataFrame(dict(additional_features))\n",
    "    train_dataset.set_transform(partial(transform_start_field, freq=freq))\n",
    "    print(time_features.head())\n",
    "    print(df.head())\n",
    "\n",
    "\n",
    "\n",
    "print(dataset)\n",
    "# ds = DatasetDict({train})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rom functools import lru_cache\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_pandas_period(date, freq):\n",
    "    return pd.Period(date, freq)\n",
    "\n",
    "\n",
    "def transform_start_field(batch, freq):\n",
    "    batch[\"start\"] = [convert_to_pandas_period(date, freq) for date in batch[\"start\"]]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'target' 열을 계산하는 함수\n",
    "from datetime import datetime, time\n",
    "\n",
    "def compute_target(row):   \n",
    "    src_start = row[\"start\"]\n",
    "    src_end = src_start + pd.Timedelta(minutes=3)\n",
    "    trg_start = src_end  # 각 행의 시간으로부터 3분 후\n",
    "    trg_end = trg_start + pd.Timedelta(seconds=30)\n",
    "\n",
    "    # trg_start과 trg_end 사이의 데이터 필터링\n",
    "    src_mask = (df['start'] >= src_start) & (df['start'] <= src_end)\n",
    "    trg_mask = (df['start'] > trg_start) & (df['start'] < trg_end)\n",
    "    \n",
    "    filtered_src = df.loc[src_mask].values if len(df.loc[src_mask]) > 0 else np.array([None])\n",
    "    filtered_trg = df.loc[trg_mask][\"per\"].values if len(df.loc[trg_mask][\"per\"]) > 0 else np.array([0.])\n",
    "\n",
    "    return filtered_trg\n",
    "\n",
    "    \n",
    "def compute_src(row):\n",
    "    src_start = row[\"start\"]\n",
    "    src_end = src_start + pd.Timedelta(minutes=3)\n",
    "\n",
    "    # trg_start과 trg_end 사이의 데이터 필터링\n",
    "    src_mask = (df['start'] >= src_start) & (df['start'] <= src_end)\n",
    "    \n",
    "    filtered_src = df.loc[src_mask].values if len(df.loc[src_mask]) > 0 else np.array([None])\n",
    "    return filtered_src\n",
    "    re[\"start\"] = re[re.index < (re.index + pd.Timedelta(seconds=180))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "2021-05-10 08:30:43    1\n",
      "2021-05-10 08:30:44    1\n",
      "2021-05-10 08:30:45    1\n",
      "2021-05-10 08:30:46    1\n",
      "2021-05-10 08:30:47    1\n",
      "                      ..\n",
      "2021-05-11 15:58:25    1\n",
      "2021-05-11 15:58:26    1\n",
      "2021-05-11 15:58:27    1\n",
      "2021-05-11 15:58:28    1\n",
      "2021-05-11 15:58:29    1\n",
      "Freq: S, Length: 113267, dtype: int64\n",
      "DatasetDict({\n",
      "    train: {}\n",
      "    validation: {}\n",
      "    test: {}\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "parquet_files = glob(os.path.join(os.path.abspath(os.path.pardir), \"data\", \"resample\", \"*.parquet\"))\n",
    "\n",
    "dataset = DatasetDict({'train': {}, 'validation': {}, \"test\": {}})\n",
    "    \n",
    "##############\n",
    "freq = \"1s\"\n",
    "lags_sequence = get_lags_for_frequency(freq)\n",
    "time_features = time_features_from_frequency_str(freq)\n",
    "##############\n",
    "\n",
    "fig = make_subplots(rows=6, cols=1)\n",
    "\n",
    "for idx, parquet_file in enumerate(parquet_files):\n",
    "    df = pd.read_parquet(parquet_file)\n",
    "    df = df.reset_index()\n",
    "    df = df.rename(columns={\"STCK_CNTG_HOUR\":\"start\"})\n",
    "    # df[\"target\"] = df.loc[df[\"start\"]:(df[\"start\"]+pd.Timedelta(minutes=3))]\n",
    "    print(df.resample(on=\"start\", rule=\"1s\").size())\n",
    "    # df[\"target\"] = df.apply(compute_target)\n",
    "    # df[\"src\"] = df.apply(compute_src)\n",
    "\n",
    "    # train, valid = train_test_split(df, test_size=0.2, random_state=32)\n",
    "    # valid, test = train_test_split(valid, test_size=0.2, random_state=32)\n",
    "\n",
    "    # dataset[\"train\"][idx] = Dataset.from_pandas(train)\n",
    "    # dataset[\"validation\"][idx] = Dataset.from_pandas(valid)\n",
    "    # dataset[\"test\"][idx] = Dataset.from_pandas(test)\n",
    "    # df.rename(columns={\"STCK_CNTG_HOUR\":\"start\"}, inplace=True)\n",
    "    # df[\"per\"] = df[\"STCK_PRPR\"] - df[\"STCK_PRPR\"].shift(fill_value=0)\n",
    "    # df[\"STCK_PRPR\"] = df[\"STCK_PRPR\"] * df[\"CNTG_VOL\"]\n",
    "    # df.rename(columns={\"STCK_PRPR\":\"amount\"}, inplace=True)\n",
    "    # src, trg = df.apply(compute_target, axis=1)\n",
    "    # print(src)\n",
    "    # print(trg)\n",
    "    break\n",
    "    # df[\"src\"] = src\n",
    "    # df[\"trg\"] = trg\n",
    "\n",
    "    # 시계열 Features\n",
    "    timestamp_as_index = pd.DatetimeIndex(df[\"start\"])\n",
    "    additional_features = [\n",
    "        (time_feature.__name__, time_feature(timestamp_as_index))\n",
    "        for time_feature in time_features\n",
    "    ]\n",
    "    \n",
    "    time_features = pd.DataFrame(dict(additional_features))\n",
    "    train_dataset.set_transform(partial(transform_start_field, freq=freq))\n",
    "    print(time_features.head())\n",
    "    print(df.head())\n",
    "\n",
    "\n",
    "\n",
    "print(dataset)\n",
    "# ds = DatasetDict({train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['start', 'open', 'high', 'low', 'close', 'CNTG_VOL', 'ask_volume', 'bid_volume', 'after_volume', 'ticks', '__index_level_0__'],\n",
       "    num_rows: 43094\n",
       "})"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sys.path.append(os.path.abspath(\"c:\\\\Users\\\\choiseokgyu\\\\Fin\"))\n",
    "# glob(os.path.join(os.path.abspath(os.path.pardir), \"data\", \"*.parquet\"))\n",
    "# ds = load_dataset(x)\n",
    "parquet_files = glob(os.path.join(os.path.abspath(os.path.pardir), \"data\", \"raw\", \"*.parquet\"))\n",
    "##############\n",
    "freq = \"1s\"\n",
    "lags_sequence = get_lags_for_frequency(freq)\n",
    "time_features = time_features_from_frequency_str(freq)\n",
    "##############\n",
    "\n",
    "fig = make_subplots(rows=6, cols=1)\n",
    "for parquet_file in parquet_files:\n",
    "    # 현재 Parquet 파일을 DataFrame으로 로드합니다.\n",
    "    df = pd.read_parquet(parquet_file)\n",
    "    # df.rename(columns={\"STCK_CNTG_HOUR\": \"start\"}, inplace=True)\n",
    "    # DataFrame을 Dataset으로 변환합니다.\n",
    "    # dataset = Dataset.from_parquet(path_or_paths=parquet_file)\n",
    "\n",
    "    prediction_length = 48\n",
    "    df.rename(columns={\"STCK_CNTG_HOUR\": \"start\"}, inplace=True)\n",
    "    df = df.assign(amount=df[\"STCK_PRPR\"] * df[\"CNTG_VOL\"])\n",
    "    timestamp_as_index = pd.DatetimeIndex(df[\"start\"])\n",
    "    additional_features = [\n",
    "        (time_feature.__name__, time_feature(timestamp_as_index))\n",
    "        for time_feature in time_features\n",
    "    ]\n",
    "    \n",
    "    # print(np.column_stack(list(dict(additional_features).values())))\n",
    "    # print(dict(additional_features).keys())\n",
    "    for idx, (key, val) in enumerate(dict(additional_features).items()):\n",
    "        # fig.add_trace(go.Scatter(x=np.arange(len(val)), y=val, mode=\"lines\", name=f\"{key}\"),row=idx+1, col=1)\n",
    "        df[key] = val\n",
    "    \n",
    "    # fig.update_layout(height=1200)\n",
    "    # fig.show()\n",
    "    df = df[df.columns.drop([\"STCK_PRPR\", \"MKSC_SHRN_ISCD\"])]\n",
    "    # print(df[df.columns.drop([\"STCK_PRPR\", \"MKSC_SHRN_ISCD\"])])\n",
    "    # print(df)\n",
    "    # df.to_parquet(parquet_file.replace(\"raw\", \"pre\"))\n",
    "    # dataset = \n",
    "    # print(df)\n",
    "\n",
    "    # 변환된 Dataset을 목록에 추가합니다.\n",
    "    # datasets_list.append(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import InformerConfig, InformerForPrediction\n",
    "\n",
    "config = InformerConfig(\n",
    "    # in the multivariate setting, input_size is the number of variates in the time series per time step\n",
    "    input_size=10997,\n",
    "    # prediction length:\n",
    "    prediction_length=prediction_length,\n",
    "    # context length:\n",
    "    context_length=prediction_length * 2,\n",
    "    # lags value copied from 1 week before:\n",
    "    lags_sequence=[1, 24 * 7],\n",
    "    # we'll add 5 time features (\"hour_of_day\", ..., and \"age\"):\n",
    "    num_time_features=len(time_features) + 1,\n",
    "    \n",
    "    # informer params:\n",
    "    dropout=0.1,\n",
    "    encoder_layers=6,\n",
    "    decoder_layers=4,\n",
    "    # project input from num_of_variates*len(lags_sequence)+num_time_features to:\n",
    "    d_model=64,\n",
    ")\n",
    "\n",
    "model = InformerForPrediction(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['MKSC_SHRN_ISCD', 'STCK_CNTG_HOUR', 'STCK_PRPR', 'CNTG_VOL', 'CCLD_DVSN', 'second_of_minute', 'minute_of_hour', 'hour_of_day', 'day_of_week', 'day_of_month', 'day_of_year'],\n",
       "    num_rows: 14288\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import lru_cache, partial\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "@lru_cache(10_000)\n",
    "def convert_to_pandas_period(date, freq):\n",
    "    print(date)\n",
    "    return pd.Period(date, freq)\n",
    "\n",
    "\n",
    "def transform_start_field(batch, freq):\n",
    "    batch[\"STCK_CNTG_HOUR\"] = [convert_to_pandas_period(date, freq) for date in batch[\"STCK_CNTG_HOUR\"]]\n",
    "    print(batch)\n",
    "    return batch\n",
    "\n",
    "parquet_files = glob(os.path.join(os.path.abspath(os.path.pardir), \"data\", \"pre\", \"*.parquet\"))\n",
    "\n",
    "datasets_list = []\n",
    "fig = make_subplots(rows=6, cols=1)\n",
    "for parquet_file in parquet_files:\n",
    "    # 현재 Parquet 파일을 DataFrame으로 로드합니다.\n",
    "    df = pd.read_parquet(parquet_file)\n",
    "    \n",
    "    # DataFrame을 Dataset으로 변환합니다.\n",
    "    dataset = Dataset.from_parquet(path_or_paths=parquet_file)\n",
    "    datasets_list.append(dataset)\n",
    "datasets_list[0].set_transform(partial(transform_start_field, freq=freq))\n",
    "datasets_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import InformerModel\n",
    "import torch\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "\n",
    "class CustomInformer(InformerModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.encoder = InformerEncoder(config)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        past_values: torch.Tensor,\n",
    "        past_time_features: torch.Tensor,\n",
    "        past_observed_mask: torch.Tensor,\n",
    "        static_categorical_features: Optional[torch.Tensor] = None,\n",
    "        static_real_features: Optional[torch.Tensor] = None,\n",
    "        future_values: Optional[torch.Tensor] = None,\n",
    "        future_time_features: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        ```python\n",
    "        >>> from huggingface_hub import hf_hub_download\n",
    "        >>> import torch\n",
    "        >>> from transformers import InformerModel\n",
    "\n",
    "        >>> file = hf_hub_download(\n",
    "        ...     repo_id=\"hf-internal-testing/tourism-monthly-batch\", filename=\"train-batch.pt\", repo_type=\"dataset\"\n",
    "        ... )\n",
    "        >>> batch = torch.load(file)\n",
    "\n",
    "        >>> model = InformerModel.from_pretrained(\"huggingface/informer-tourism-monthly\")\n",
    "\n",
    "        >>> # during training, one provides both past and future values\n",
    "        >>> # as well as possible additional features\n",
    "        >>> outputs = model(\n",
    "        ...     past_values=batch[\"past_values\"],\n",
    "        ...     past_time_features=batch[\"past_time_features\"],\n",
    "        ...     past_observed_mask=batch[\"past_observed_mask\"],\n",
    "        ...     static_categorical_features=batch[\"static_categorical_features\"],\n",
    "        ...     static_real_features=batch[\"static_real_features\"],\n",
    "        ...     future_values=batch[\"future_values\"],\n",
    "        ...     future_time_features=batch[\"future_time_features\"],\n",
    "        ... )\n",
    "\n",
    "        >>> last_hidden_state = outputs.last_hidden_state\n",
    "        ```\"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        transformer_inputs, loc, scale, static_feat = self.create_network_inputs(\n",
    "            past_values=past_values,\n",
    "            past_time_features=past_time_features,\n",
    "            past_observed_mask=past_observed_mask,\n",
    "            static_categorical_features=static_categorical_features,\n",
    "            static_real_features=static_real_features,\n",
    "            future_values=future_values,\n",
    "            future_time_features=future_time_features,\n",
    "        )\n",
    "\n",
    "        if encoder_outputs is None:\n",
    "            enc_input = transformer_inputs[:, : self.config.context_length, ...]\n",
    "            encoder_outputs = self.encoder(\n",
    "                inputs_embeds=enc_input,\n",
    "                head_mask=head_mask,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "            \n",
    "        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\n",
    "        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n",
    "            encoder_outputs = BaseModelOutput(\n",
    "                last_hidden_state=encoder_outputs[0],\n",
    "                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
    "                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n",
    "            )\n",
    "\n",
    "        return encoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 3 and 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m CustomInformer(config)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpast_values\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpast_time_features\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpast_observed_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstatic_categorical_features\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstatic_real_features\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfuture_values\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfuture_time_features\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\choiseokgyu\\Fin\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\choiseokgyu\\Fin\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[35], line 65\u001b[0m, in \u001b[0;36mCustomInformer.forward\u001b[1;34m(self, past_values, past_time_features, past_observed_mask, static_categorical_features, static_real_features, future_values, future_time_features, head_mask, encoder_outputs, output_hidden_states, output_attentions, use_cache, return_dict)\u001b[0m\n\u001b[0;32m     62\u001b[0m use_cache \u001b[38;5;241m=\u001b[39m use_cache \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache\n\u001b[0;32m     63\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m---> 65\u001b[0m transformer_inputs, loc, scale, static_feat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_network_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_time_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_time_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_observed_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_observed_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatic_categorical_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatic_categorical_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatic_real_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatic_real_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfuture_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuture_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfuture_time_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuture_time_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m     enc_input \u001b[38;5;241m=\u001b[39m transformer_inputs[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mcontext_length, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\choiseokgyu\\Fin\\.venv\\Lib\\site-packages\\transformers\\models\\informer\\modeling_informer.py:1550\u001b[0m, in \u001b[0;36mInformerModel.create_network_inputs\u001b[1;34m(self, past_values, past_time_features, static_categorical_features, static_real_features, past_observed_mask, future_values, future_time_features)\u001b[0m\n\u001b[0;32m   1547\u001b[0m static_feat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((log_abs_loc, log_scale), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m static_real_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1550\u001b[0m     static_feat \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatic_real_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatic_feat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m static_categorical_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1552\u001b[0m     embedded_cat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedder(static_categorical_features)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 3 and 2"
     ]
    }
   ],
   "source": [
    "model = CustomInformer(config)\n",
    "model(**{\n",
    "    \"past_values\": torch.rand(1, 10, 9),\n",
    "    \"past_time_features\": torch.rand(1, 10, 6),\n",
    "    \"past_observed_mask\": torch.rand(1, 9),\n",
    "    \"static_categorical_features\": torch.rand(1, 10, 1),\n",
    "    \"static_real_features\": torch.rand(1, 10, 1),\n",
    "    \"future_values\": torch.rand(1, 10, 9),\n",
    "    \"future_time_features\": torch.rand(1, 10, 6),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "\n",
    "def probsparse_attention(query_states, key_states, value_states, sampling_factor=5):\n",
    "    \"\"\"\n",
    "    Compute the probsparse self-attention.\n",
    "    Input shape: Batch x Time x Channel\n",
    "\n",
    "    Note the additional `sampling_factor` input.\n",
    "    \"\"\"\n",
    "    # get input sizes with logs\n",
    "    L_K = key_states.size(1)\n",
    "    L_Q = query_states.size(1)\n",
    "    log_L_K = np.ceil(np.log1p(L_K)).astype(\"int\").item()\n",
    "    log_L_Q = np.ceil(np.log1p(L_Q)).astype(\"int\").item()\n",
    "\n",
    "    # calculate a subset of samples to slice from K and create Q_K_sample\n",
    "    U_part = min(sampling_factor * L_Q * log_L_K, L_K)\n",
    "\n",
    "    # create Q_K_sample (the q_i * k_j^T term in the sparsity measurement)\n",
    "    index_sample = torch.randint(0, L_K, (U_part,))\n",
    "    K_sample = key_states[:, index_sample, :]\n",
    "    Q_K_sample = torch.bmm(query_states, K_sample.transpose(1, 2))\n",
    "\n",
    "    # calculate the query sparsity measurement with Q_K_sample\n",
    "    M = Q_K_sample.max(dim=-1)[0] - torch.div(Q_K_sample.sum(dim=-1), L_K)\n",
    "\n",
    "    # calculate u to find the Top-u queries under the sparsity measurement\n",
    "    u = min(sampling_factor * log_L_Q, L_Q)\n",
    "    M_top = M.topk(u, sorted=False)[1]\n",
    "\n",
    "    # calculate Q_reduce as query_states[:, M_top]\n",
    "    dim_for_slice = torch.arange(query_states.size(0)).unsqueeze(-1)\n",
    "    Q_reduce = query_states[dim_for_slice, M_top]  # size: c*log_L_Q x channel\n",
    "\n",
    "    # and now, same as the canonical\n",
    "    d_k = query_states.size(-1)\n",
    "    attn_scores = torch.bmm(Q_reduce, key_states.transpose(-2, -1))  # Q_reduce x K^T\n",
    "    attn_scores = attn_scores / math.sqrt(d_k)\n",
    "    attn_probs = nn.functional.softmax(attn_scores, dim=-1)\n",
    "    attn_output = torch.bmm(attn_probs, value_states)\n",
    "\n",
    "    return attn_output, attn_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConvLayer is a class with forward pass applying ELU and MaxPool1d\n",
    "def informer_encoder_forward(x_input, num_encoder_layers=3, distil=True):\n",
    "    # Initialize the convolution layers\n",
    "    if distil:\n",
    "        conv_layers = nn.ModuleList([ConvLayer() for _ in range(num_encoder_layers - 1)])\n",
    "        conv_layers.append(None)\n",
    "    else:\n",
    "        conv_layers = [None] * num_encoder_layers\n",
    "    \n",
    "    # Apply conv_layer between each encoder_layer\n",
    "    for encoder_layer, conv_layer in zip(encoder_layers, conv_layers):\n",
    "        output = encoder_layer(x_input)\n",
    "        if conv_layer is not None:\n",
    "            output = conv_layer(loutput)\n",
    "    \n",
    "    return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
